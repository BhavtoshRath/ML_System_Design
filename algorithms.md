### Popular Machine Learning Algorithms and When to Use Them


| **Algorithm**         | **Type**         | **When to Use** |
|----------------------|----------------|----------------|
| **Linear Regression** | Supervised (Regression) | - When the relationship between input and output is linear. <br> - When interpretability is important. <br> - For predicting continuous values (e.g., house prices, sales forecasting). |
| **Logistic Regression** | Supervised (Classification) | - When predicting probabilities or binary outcomes (e.g., spam detection, medical diagnosis). <br> - When interpretability is needed. |
| **Decision Trees** | Supervised (Classification & Regression) | - When interpretability is needed. <br> - When handling non-linear relationships. <br> - Works well with small to medium datasets. |
| **Random Forest** | Supervised (Classification & Regression) | - When high accuracy is needed. <br> - When handling missing data and reducing overfitting. <br> - When working with structured/tabular data. |
| **Gradient Boosting (XGBoost, LightGBM, CatBoost)** | Supervised (Classification & Regression) | - When high performance is required. <br> - Works well with large, structured/tabular datasets. <br> - When handling missing values and feature importance is needed. |
| **Support Vector Machines (SVM)** | Supervised (Classification & Regression) | - When working with small to medium-sized datasets. <br> - When data is not linearly separable. <br> - When using kernel methods for complex decision boundaries. |
| **k-Nearest Neighbors (KNN)** | Supervised (Classification & Regression) | - When data is small and memory-based learning is acceptable. <br> - When decision boundaries are non-linear. <br> - When interpretability is not critical. |
| **Naive Bayes** | Supervised (Classification) | - When working with text classification (e.g., spam filtering, sentiment analysis). <br> - When independence assumptions between features hold. <br> - Works well with small datasets. |
| **K-Means Clustering** | Unsupervised (Clustering) | - When grouping data into clusters without labels. <br> - When data is well-separated. <br> - Used for customer segmentation, anomaly detection. |
| **DBSCAN (Density-Based Clustering)** | Unsupervised (Clustering) | - When detecting clusters of varying shapes. <br> - When handling noise and outliers well. <br> - Works well in spatial data analysis. |
| **PCA (Principal Component Analysis)** | Unsupervised (Dimensionality Reduction) | - When reducing feature space while preserving variance. <br> - When working with high-dimensional data (e.g., image compression, genomics). |
| **Autoencoders** | Unsupervised (Dimensionality Reduction & Anomaly Detection) | - When detecting anomalies in high-dimensional data. <br> - When learning compressed representations of input data. |
| **Neural Networks (Deep Learning)** | Supervised (Classification & Regression) | - When working with large amounts of data. <br> - When handling complex patterns (e.g., image recognition, NLP). <br> - When interpretability is not a priority. |
| **CNN (Convolutional Neural Networks)** | Supervised (Deep Learning) | - When processing image data (e.g., object detection, face recognition). <br> - When detecting spatial features in images. |
| **RNN (Recurrent Neural Networks) & LSTMs** | Supervised (Deep Learning) | - When working with sequential data (e.g., time series forecasting, speech recognition). <br> - When handling long-term dependencies. |
| **Transformers (BERT, GPT, etc.)** | Supervised (Deep Learning - NLP) | - When working with Natural Language Processing tasks (e.g., text generation, translation). <br> - When handling long text sequences. |
| **Reinforcement Learning (DQN, PPO, A3C, etc.)** | Reinforcement Learning | - When optimizing decision-making in dynamic environments (e.g., robotics, game playing). <br> - When learning from trial and error. |
